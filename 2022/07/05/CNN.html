<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>mnist 데이터를 이용한 multi_layers 학습 | sohee🌸</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="mnist 데이터를 이용한 multi_layers 학습" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://pythonhee.github.io/cosmos/2022/07/05/CNN.html" />
<meta property="og:url" content="https://pythonhee.github.io/cosmos/2022/07/05/CNN.html" />
<meta property="og:site_name" content="sohee🌸" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-05T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="mnist 데이터를 이용한 multi_layers 학습" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-05T00:00:00-05:00","datePublished":"2022-07-05T00:00:00-05:00","description":"An easy to use blogging platform with support for Jupyter Notebooks.","headline":"mnist 데이터를 이용한 multi_layers 학습","mainEntityOfPage":{"@type":"WebPage","@id":"https://pythonhee.github.io/cosmos/2022/07/05/CNN.html"},"url":"https://pythonhee.github.io/cosmos/2022/07/05/CNN.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/cosmos/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://pythonhee.github.io/cosmos/feed.xml" title="sohee🌸" /><link rel="shortcut icon" type="image/x-icon" href="/cosmos/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/cosmos/">sohee🌸</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/cosmos/about/">About Me</a><a class="page-link" href="/cosmos/search/">Search</a><a class="page-link" href="/cosmos/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">mnist 데이터를 이용한 multi_layers 학습</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-05T00:00:00-05:00" itemprop="datePublished">
        Jul 5, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/pythonhee/cosmos/tree/master/_notebooks/2022-07-05-CNN.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/cosmos/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/pythonhee/cosmos/master?filepath=_notebooks%2F2022-07-05-CNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/cosmos/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/pythonhee/cosmos/blob/master/_notebooks/2022-07-05-CNN.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/cosmos/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fpythonhee%2Fcosmos%2Fblob%2Fmaster%2F_notebooks%2F2022-07-05-CNN.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/cosmos/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#12_1_single_layer">12_1_single_layer </a></li>
<li class="toc-entry toc-h2"><a href="#12_2_multi_layers">12_2_multi_layers </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-07-05-CNN.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="12_1_single_layer">
<a class="anchor" href="#12_1_single_layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>12_1_single_layer<a class="anchor-link" href="#12_1_single_layer"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Dense 함수와 여러개의 초기화 값 정의하기</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">Dense</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

  <span class="k">return</span> <span class="n">activations</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="k">if</span> <span class="n">activations</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_weight_normal</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">]))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">([</span><span class="n">n_output</span><span class="p">]))</span>

  <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">make_weight_glorot</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
  <span class="n">glorot</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">GlorotUniform</span><span class="p">()</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">glorot</span><span class="p">([</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">]))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_output</span><span class="p">]))</span>

  <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">make_weight_he</span><span class="p">(</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">):</span>
  <span class="n">he</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">HeUniform</span><span class="p">()</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">he</span><span class="p">([</span><span class="n">n_input</span><span class="p">,</span> <span class="n">n_output</span><span class="p">]))</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_output</span><span class="p">]))</span>

  <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>mnist_single_layer 함수 생성</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">mnist_single_layer</span><span class="p">():</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000, 28, 28) (10000, 28, 28) </span>
  <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000,) (10000,) =&gt; y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>   <span class="c1"># 0, 255</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>

<span class="c1"># x데이터를 0~1 값으로 정규화시켜주는 작업.</span>

  <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>                              <span class="c1"># n_output 값</span>

  <span class="c1"># w, b = make_weight_normal(x_train.shape[-1], n_classes)</span>
  <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">make_weight_glorot</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_classes</span><span class="p">)</span>
  <span class="c1"># w, b = make_weight_he(x_train.shape[-1], n_classes)</span>

  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
      <span class="n">hx</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

      <span class="n">scce</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">scce</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>

    <span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="nb">print</span><span class="p">()</span>  

  <span class="n">p</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                <span class="c1">#(10000, 10)</span>

  <span class="n">p_arg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="n">p_arg</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">'acc:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_arg</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">mnist_single_layer</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(60000, 28, 28) (10000, 28, 28)
(60000,) (10000,)
0 255
0 2.4037037
1 1.9596609
2 1.599312
3 1.3130828
4 1.1049172
5 0.95262057
6 0.83855027
7 0.7554665
8 0.69375753
9 0.64428
10 0.60332835
11 0.5701764
12 0.54368937
13 0.5217784
14 0.502841
15 0.48651809
16 0.47281685
17 0.4612233
18 0.45088997
19 0.44128928
20 0.43242112
21 0.42447037
22 0.41741672
23 0.41099578
24 0.4049556
25 0.39925858
26 0.39401338
27 0.3892681
28 0.38491824
29 0.38080114
30 0.376837
31 0.37305978
32 0.36953098
33 0.36624712
34 0.36313674
35 0.36013457
36 0.35723847
37 0.35448992
38 0.35190997
39 0.34946993
40 0.34712353
41 0.34485543
42 0.34268677
43 0.34063777
44 0.33869678
45 0.33683166
46 0.33502424
47 0.33328217
48 0.33161727
49 0.33002266
50 0.32847792
51 0.32697153
52 0.32550958
53 0.32410192
54 0.32274655
55 0.32143202
56 0.32015175
57 0.3189091
58 0.317709
59 0.31654832
60 0.31541887
61 0.3143168
62 0.3132445
63 0.31220457
64 0.31119463
65 0.31020984
66 0.30924878
67 0.30831352
68 0.30740508
69 0.30652076
70 0.30565703
71 0.3048132
72 0.30399042
73 0.3031882
74 0.30240428
75 0.30163667
76 0.3008854
77 0.3001511
78 0.29943293
79 0.2987289
80 0.2980381
81 0.29736075
82 0.29669678
83 0.29604524
84 0.29540515
85 0.29477626
86 0.2941588
87 0.2935524
88 0.29295632
89 0.29236993
90 0.2917933
91 0.29122633
92 0.2906685
93 0.2901193
94 0.28957856
95 0.28904623
96 0.28852212
97 0.28800583
98 0.2874971
99 0.28699583

(10000, 10)
[7 2 1 ... 4 5 6]
acc: 0.9208
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>mini_batch 사용해 생성</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">mnist_single_layer_mini_batch</span><span class="p">():</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000, 28, 28) (10000, 28, 28) </span>
  <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000,) (10000,) =&gt; y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>   <span class="c1"># 0, 255</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>

<span class="c1"># x데이터를 0~1 값으로 정규화시켜주는 작업.</span>

  <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>                              <span class="c1"># n_output 값</span>

  <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_classes</span><span class="p">)</span>

  <span class="c1"># w, b = make_weight_glorot(x_train.shape[-1], n_classes)</span>
  <span class="c1"># acc: 0.4583</span>
  
  <span class="c1"># w, b = make_weight_he(x_train.shape[-1], n_classes)</span>

  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!</span>

  <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10</span>                               <span class="c1"># 60000데이터를 10회 반복한다는 의미 </span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="n">n_literation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>   <span class="c1"># 600</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_literation</span><span class="p">):</span>              <span class="c1"># 6000번이나 반복 확률이 up</span>
      <span class="n">n1</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">batch_size</span>
      <span class="n">n2</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">*</span> <span class="n">batch_size</span>

      <span class="n">xx</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>
      <span class="n">yy</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">hx</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

        <span class="n">scce</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">scce</span><span class="p">(</span><span class="n">yy</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total</span> <span class="o">/</span> <span class="n">n_literation</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">()</span>  

  <span class="n">p</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                <span class="c1">#(10000, 10)</span>

  <span class="n">p_arg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="n">p_arg</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">'acc:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_arg</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">mnist_single_layer_mini_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
11501568/11490434 [==============================] - 0s 0us/step
(60000, 28, 28) (10000, 28, 28)
(60000,) (10000,)
0 255
0 0.31139886550294854
1 0.1945932127473255
2 0.17942959094420075
3 0.17137116949539632
4 0.1662351022940129
5 0.16266893739035973
6 0.16009065205580555
7 0.15819382834791515
8 0.15678756201523356
9 0.1557471169391647

(10000, 10)
[7 2 1 ... 4 5 6]
acc: 0.899
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="12_2_multi_layers">
<a class="anchor" href="#12_2_multi_layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>12_2_multi_layers<a class="anchor-link" href="#12_2_multi_layers"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>layer 3개</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">mnist_multiple_layer_mini_batch_3</span><span class="p">():</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000, 28, 28) (10000, 28, 28) </span>
  <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000,) (10000,) =&gt; y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>   <span class="c1"># 0, 255</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>

<span class="c1"># x데이터를 0~1 값으로 정규화시켜주는 작업.</span>

  <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>                              <span class="c1"># n_output 값</span>

  <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">256</span><span class="p">)</span>
  <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
  <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
  <span class="c1"># acc: 0.9423</span>

  <span class="c1"># w1, b1 = make_weight_glorot(x_train.shape[-1], 256)</span>
  <span class="c1"># w2, b2 = make_weight_glorot(256, 256)</span>
  <span class="c1"># w3, b3 = make_weight_glorot(256, n_classes)</span>
  <span class="c1"># 결과 :</span>

  <span class="c1"># w1, b1 = make_weight_he(x_train.shape[-1], 256)</span>
  <span class="c1"># w2, b2 = make_weight_he(256, 256)</span>
  <span class="c1"># w3, b3 = make_weight_he(256, n_classes)</span>
  <span class="c1"># 결과 :</span>


  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!</span>

  <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10</span>                               <span class="c1"># 60000데이터를 10회 반복한다는 의미 </span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="n">n_literation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>   <span class="c1"># 600</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_literation</span><span class="p">):</span>              <span class="c1"># 6000번이나 반복 확률이 up</span>
      <span class="n">n1</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">batch_size</span>
      <span class="n">n2</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">*</span> <span class="n">batch_size</span>

      <span class="n">xx</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>
      <span class="n">yy</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">hx</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

  <span class="c1"># layer끼리 연결할 때 activation은 non-linear해야한다. </span>
  <span class="c1"># sigmoid ,  softmax 는 예측 x = &gt; 마지막에만 확률로 변경하는 sigmoid, softmax 사용</span>
  <span class="c1"># relu 는 0보다 작은 값은 0으로, 0보다 큰 값은 그대로 나타냄 0보다 작은 값이 많은 데이터는 사용하기 어려움.</span>

        <span class="n">scce</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">scce</span><span class="p">(</span><span class="n">yy</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">])</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total</span> <span class="o">/</span> <span class="n">n_literation</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">()</span>  

  <span class="n">d1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">d2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                <span class="c1">#(10000, 10)</span>

  <span class="n">p_arg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">'acc:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_arg</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">mnist_multiple_layer_mini_batch_3</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(60000, 28, 28) (10000, 28, 28)
(60000,) (10000,)
0 255
0 1461.6672620370766
1 1.1854659057674386
2 0.3881142702441624
3 0.15620899812240774
4 0.06981539531475088
5 0.03489897323093222
6 3.321667264445374
7 0.14723950815076628
8 0.09502428941739102
9 0.06494564253681649

(10000, 10)
acc: 0.9423
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Q</code> layer 5개로 늘려 정확도를 구하고 layer 3개를 이용했을 때와 비교하시오</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">mnist_multiple_layer_mini_batch_5</span><span class="p">():</span>
  <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000, 28, 28) (10000, 28, 28) </span>
  <span class="nb">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        <span class="c1"># (60000,) (10000,) =&gt; y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>   <span class="c1"># 0, 255</span>

  <span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">/</span> <span class="mi">255</span>
  <span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span> <span class="o">/</span> <span class="mi">255</span>

<span class="c1"># x데이터를 0~1 값으로 정규화시켜주는 작업.</span>

  <span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>                              <span class="c1"># n_output 값</span>

  <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">256</span><span class="p">)</span>
  <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
  <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">w4</span><span class="p">,</span> <span class="n">b4</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">w5</span><span class="p">,</span> <span class="n">b5</span> <span class="o">=</span> <span class="n">make_weight_normal</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
  <span class="c1"># acc: 0.8988 </span>

  <span class="c1"># w1, b1 = make_weight_glorot(x_train.shape[-1], 256)</span>
  <span class="c1"># w2, b2 = make_weight_glorot(256, 64)</span>
  <span class="c1"># w3, b3 = make_weight_glorot(64, 32)</span>
  <span class="c1"># w4, b4 = make_weight_glorot(32, 32)</span>
  <span class="c1"># w5, b5 = make_weight_glorot(32, n_classes)</span>
  <span class="c1"># 결과 :</span>

  <span class="c1"># w1, b1 = make_weight_he(x_train.shape[-1], 256)</span>
  <span class="c1"># w2, b2 = make_weight_he(256, 64)</span>
  <span class="c1"># w3, b3 = make_weight_he(64, 32)</span>
  <span class="c1"># w4, b4 = make_weight_he(32, 32)</span>
  <span class="c1"># w5, b5 = make_weight_he(32, n_classes)</span>
  <span class="c1"># 결과 :</span>


  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!</span>

  <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10</span>                               <span class="c1"># 60000데이터를 10회 반복한다는 의미 </span>
  <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
  <span class="n">n_literation</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>   <span class="c1"># 600</span>

  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_literation</span><span class="p">):</span>              <span class="c1"># 6000번이나 반복 확률이 up</span>
      <span class="n">n1</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">batch_size</span>
      <span class="n">n2</span> <span class="o">=</span> <span class="n">n1</span> <span class="o">*</span> <span class="n">batch_size</span>

      <span class="n">xx</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>
      <span class="n">yy</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">n1</span><span class="p">:</span><span class="n">n2</span><span class="p">]</span>

      <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">d3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">d4</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d3</span><span class="p">,</span> <span class="n">w4</span><span class="p">,</span> <span class="n">b4</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
        <span class="n">hx</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d4</span><span class="p">,</span> <span class="n">w5</span><span class="p">,</span> <span class="n">b5</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>

        <span class="n">scce</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">scce</span><span class="p">(</span><span class="n">yy</span><span class="p">,</span> <span class="n">hx</span><span class="p">)</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

      <span class="n">gradient</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">w4</span><span class="p">,</span> <span class="n">b4</span><span class="p">,</span> <span class="n">w5</span><span class="p">,</span> <span class="n">b5</span><span class="p">])</span>
      <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="p">[</span><span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">w4</span><span class="p">,</span> <span class="n">b4</span><span class="p">,</span> <span class="n">w5</span><span class="p">,</span> <span class="n">b5</span><span class="p">]))</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">total</span> <span class="o">/</span> <span class="n">n_literation</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">()</span>  

  <span class="n">d1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">d2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">d3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d2</span><span class="p">,</span> <span class="n">w3</span><span class="p">,</span> <span class="n">b3</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">d4</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d3</span><span class="p">,</span> <span class="n">w4</span><span class="p">,</span> <span class="n">b4</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">relu</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">d4</span><span class="p">,</span> <span class="n">w5</span><span class="p">,</span> <span class="n">b5</span><span class="p">,</span> <span class="n">activations</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>                <span class="c1">#(10000, 10)</span>

  <span class="n">p_arg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s1">'acc:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">p_arg</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">mnist_multiple_layer_mini_batch_5</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(60000, 28, 28) (10000, 28, 28)
(60000,) (10000,)
0 255
0 115397.61487060547
1 88.93705628871918
2 124.64481646577518
3 8.39682581782341
4 95.10231856604418
5 5.290038377592961
6 3.5611470832582564
7 8.595668331881365
8 25.354680725370223
9 1.1293716185198477

(10000, 10)
acc: 0.8988
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>Q</code> normal 버전 말고 glorot, he 버전도 돌려보면서 정확도를 측정해 보시오</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="pythonhee/cosmos"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/cosmos/2022/07/05/CNN.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/cosmos/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/cosmos/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/cosmos/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" target="_blank" title="fastai"><svg class="svg-icon grey"><use xlink:href="/cosmos/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
