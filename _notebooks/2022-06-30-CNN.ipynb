{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-06-30-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# logistic regression 함수 생성 및 indian 프로젝트\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: \n",
        "\n"
      ],
      "metadata": {
        "id": "CmuoPSXms_Z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9_1_logistic_regression\n",
        "\n"
      ],
      "metadata": {
        "id": "9hbeJ2wZBpxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sigmoid 배우기"
      ],
      "metadata": {
        "id": "Z0ehmZA-B7fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "k8skoxrNttBc"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.e ** -z)"
      ],
      "metadata": {
        "id": "_CKPKIA3ByHp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sigmoid():\n",
        "  for z in np.linspace(-10, 10):\n",
        "    s = sigmoid(z)\n",
        "\n",
        "    plt.plot(z, s, 'ro')\n",
        "  plt.show()\n",
        "\n",
        "show_sigmoid()      # Dense가 직선이 아닌 곡선으로 표현되어 더 유연하게 값을 찾아낼 수 있다. \n",
        "\n",
        "# linear == 직선 1개        => 2개 중 1개 고르기 \n",
        "# multiple == 직선 2개이상  => 여러개 중 1개 고르기 \n",
        "# sigmoid == 곡선 1개       => 2개 중 1개 고르기 \n",
        "# softmax == 곡선 2개이상   => 여러개 중 1개 고르기 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "nZPW9rxDpKhp",
        "outputId": "4c855979-85b8-4ed7-c3f1-78fc57963a5b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- binary_cross_entropy() 함수 생성"
      ],
      "metadata": {
        "id": "duL8ob1_MdLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y, p): \n",
        "  loss_i = y * -tf.math.log(p) + (1 - y) * -tf.math.log(1 - p)\n",
        "  return tf.reduce_mean(loss_i)\n",
        "\n",
        "# mean_sqaure_error 대신사용\n",
        "# 0과 1로 이뤄어져 sigmoid에 사용하기 좋다."
      ],
      "metadata": {
        "id": "Undz8ih7qPvR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression():\n",
        "  def Dense(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "  x = [[1, 2],         # 학습시간, 출석일수 (6, 2)\n",
        "      [2, 1],\n",
        "      [4, 5],\n",
        "      [5, 4],\n",
        "      [8, 9],\n",
        "      [9, 8]] \n",
        "\n",
        "  y = [[0],               \n",
        "       [0],\n",
        "       [1],             # 0 == 탈락, 1 == 통과 (6, 1)\n",
        "       [1],\n",
        "       [1],\n",
        "       [1]] \n",
        "\n",
        "  y = np.int32(y)       # y 데이터가 반드시 숫자여야 하기 때문에 숫자로 지정해준다. \n",
        "\n",
        "  w = tf.Variable(tf.random.uniform([2, 1]))   # (6, 1) = (6, 2) * (2, 1)\n",
        "  b = tf.Variable(tf.random.uniform([1]))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(0.1)\n",
        "\n",
        "  for i in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "      z = Dense(x, w, b)\n",
        "      hx = keras.activations.sigmoid(z)\n",
        "\n",
        "      loss = keras.losses.binary_crossentropy(y, hx, axis = 0)\n",
        "\n",
        "    gradient1 = tape.gradient(loss, [w, b])\n",
        "    optimizer.apply_gradients(zip(gradient1, [w, b]))\n",
        "\n",
        "    print('loss', i,':', loss.numpy())\n",
        "\n",
        "    z = Dense(x, w, b)\n",
        "    p = sigmoid(z).numpy()\n",
        "    \n",
        "    p_flat = p.reshape(-1)\n",
        "    print(p_flat)\n",
        "\n",
        "    p_bool = np.int32(p_flat > 0.5)\n",
        "    y_flat = y.reshape(-1)\n",
        "    print(p_bool)\n",
        "    print(y_flat)\n",
        "\n",
        "    equals = (p_bool == y_flat)\n",
        "    print(equals)\n",
        "    print('acc :', np.mean(equals))\n",
        "    print('-' * 30)\n",
        "\n",
        "logistic_regression()\n"
      ],
      "metadata": {
        "id": "Rt0OpOMoMYCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea013bf-7ac8-4a1a-f2c2-4fa5feed0687"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0 : [0.9959554]\n",
            "[0.9287879  0.95044255 0.9990939  0.99938357 0.9999975  0.99999833]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 1 : [0.94137377]\n",
            "[0.9165606  0.9416799  0.99857545 0.9990305  0.9999944  0.9999962 ]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 2 : [0.8879748]\n",
            "[0.90264493 0.931614   0.99777395 0.99848396 0.99998736 0.9999914 ]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 3 : [0.8359575]\n",
            "[0.8869527  0.9201419  0.99654657 0.9976458  0.9999716  0.9999807 ]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 4 : [0.78555274]\n",
            "[0.8694433  0.9071872  0.9946885  0.996375   0.99993753 0.99995744]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 5 : [0.7370301]\n",
            "[0.8501429  0.8927156  0.99191684 0.99447495 0.9998648  0.99990785]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 6 : [0.69070476]\n",
            "[0.82916564 0.87675303 0.9878565  0.9916825  0.9997135  0.99980456]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 7 : [0.6469464]\n",
            "[0.8067355  0.8594087  0.9820435  0.98766774 0.99940836 0.99959594]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 8 : [0.6061864]\n",
            "[0.7832069  0.84089863 0.9739604  0.98205316 0.9988166  0.99919075]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n",
            "loss 9 : [0.568919]\n",
            "[0.75907934 0.8215687  0.9631308  0.9744736  0.997724   0.9984414 ]\n",
            "[1 1 1 1 1 1]\n",
            "[0 0 1 1 1 1]\n",
            "[False False  True  True  True  True]\n",
            "acc : 0.6666666666666666\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9_2_logistic_regression_indian"
      ],
      "metadata": {
        "id": "KQ0P9Fy29vlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q` pima-indians-diabetes.csv 파일을 읽어 당뇨병을 판단하는 모델을 구축하시오\n",
        "- 정확도를 표시하시오\n"
      ],
      "metadata": {
        "id": "E2I1-AvR92Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "kaKzh-yG8VAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indian = pd.read_csv('/content/pima-indians-diabetes.csv', skiprows = 9, header = None)\n",
        "\n",
        "x = indian.values[: , :-1]         #(768, 8)\n",
        "y = indian.values[: , -1:]         #(768, 1)\n",
        "       \n",
        "y = np.int32(y)\n",
        "\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9zwJMrZh1xn",
        "outputId": "f750318e-f4de-43cb-aa05-7ec1c71e154b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 8) (768, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_indian():\n",
        "  def Dense(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "  w = tf.Variable(tf.random.uniform([8, 1]))   # (768, 1) = (768, 8) * (8, 1)\n",
        "  b = tf.Variable(tf.random.uniform([1]))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(0.001)\n",
        "\n",
        "  for i in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "      z = Dense(x, w, b)\n",
        "      hx = keras.activations.sigmoid(z)\n",
        "\n",
        "      bce = keras.losses.BinaryCrossentropy()\n",
        "      loss = bce(y, hx)\n",
        "\n",
        "    gradient1 = tape.gradient(loss, [w, b])\n",
        "    optimizer.apply_gradients(zip(gradient1, [w, b]))\n",
        "\n",
        "    print('loss', i,':', loss.numpy())\n",
        "\n",
        "    z = Dense(x, w, b)\n",
        "    p = keras.activations.sigmoid(z).numpy()\n",
        "    \n",
        "    p_flat = (p > 0.5).astype(np.int32).reshape(-1)\n",
        "    y_flat = y.reshape(-1)\n",
        "\n",
        "    print('acc :', np.mean(p_flat == y_flat))\n",
        "    print('-' * 30)\n",
        "\n",
        "logistic_regression_indian()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrPqSR0Fv076",
        "outputId": "d5aeb407-2b8a-420c-bc59-ac447ff6a963"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0 : 100.299644\n",
            "acc : 0.3489583333333333\n",
            "------------------------------\n",
            "loss 1 : 90.22592\n",
            "acc : 0.3489583333333333\n",
            "------------------------------\n",
            "loss 2 : 80.1522\n",
            "acc : 0.3489583333333333\n",
            "------------------------------\n",
            "loss 3 : 70.07848\n",
            "acc : 0.3489583333333333\n",
            "------------------------------\n",
            "loss 4 : 60.004757\n",
            "acc : 0.3489583333333333\n",
            "------------------------------\n",
            "loss 5 : 50.08167\n",
            "acc : 0.3515625\n",
            "------------------------------\n",
            "loss 6 : 40.998386\n",
            "acc : 0.35546875\n",
            "------------------------------\n",
            "loss 7 : 33.329323\n",
            "acc : 0.35546875\n",
            "------------------------------\n",
            "loss 8 : 26.595907\n",
            "acc : 0.3763020833333333\n",
            "------------------------------\n",
            "loss 9 : 21.281498\n",
            "acc : 0.37890625\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9_3_logistic_regression_indian_split"
      ],
      "metadata": {
        "id": "l7rLxJ-BxFPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q` 70% 데이터로 학습하고 30% 데이터에 대해 정확도를 계산하시오"
      ],
      "metadata": {
        "id": "WytOB7mLxLMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "BoEvZrgexyfc"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5ecUQZ3yHmB",
        "outputId": "c07a6bd3-175d-4233-dff4-9ecee90cbd77"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  6.    148.     72.    ...  33.6     0.627  50.   ]\n",
            " [  1.     85.     66.    ...  26.6     0.351  31.   ]\n",
            " [  8.    183.     64.    ...  23.3     0.672  32.   ]\n",
            " ...\n",
            " [  5.    121.     72.    ...  26.2     0.245  30.   ]\n",
            " [  1.    126.     60.    ...  30.1     0.349  47.   ]\n",
            " [  1.     93.     70.    ...  30.4     0.315  23.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_indian():\n",
        "    def dense(x, w, b):\n",
        "        # (768, 1) = (768, 8) @ (8, 1)\n",
        "        # (600, 1) = (600, 8) @ (8, 1)\n",
        "        # (168, 1) = (168, 8) @ (8, 1)\n",
        "        return x @ w + b\n",
        "\n",
        "    x = preprocessing.scale(x)              # 표준화. \n",
        "    # x = preprocessing.minmax_scale(x)     # 정규화. \n",
        "    \n",
        "    train_size = int(len(x) * 0.7)          # train_size = 600\n",
        "\n",
        "    x_train, x_test = x[:train_size], x[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    print(x_train.shape, x_test.shape)      # (537, 8) (231, 8)\n",
        "    print(y_train.shape, y_test.shape)      # (537, 1) (231, 1)\n",
        "\n",
        "    w = tf.Variable(tf.random.uniform([8, 1]))\n",
        "    b = tf.Variable(tf.random.uniform([1]))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "    for i in range(1000):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z = dense(x_train, w, b)\n",
        "            hx = keras.activations.sigmoid(z)\n",
        "\n",
        "            bce = keras.losses.BinaryCrossentropy()\n",
        "            loss = bce(y_train, hx)\n",
        "\n",
        "        gradient = tape.gradient(loss, [w, b])\n",
        "        optimizer.apply_gradients(zip(gradient, [w, b]))\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i, loss.numpy())\n",
        "    print()\n",
        "\n",
        "    z = dense(x_test, w, b)\n",
        "    p = keras.activations.sigmoid(z).numpy()\n",
        "    # print(p)\n",
        "\n",
        "    p_flat = (p > 0.5).astype(np.int32).reshape(-1)\n",
        "    y_flat = y_test.reshape(-1)\n",
        "    print(p_flat[:10])\n",
        "    print(y_flat[:10])\n",
        "\n",
        "    print('acc :', np.mean(p_flat == y_flat))\n",
        "    \n",
        "\n",
        "\n",
        "logistic_regression_indian()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "Huw44VVgxY4C",
        "outputId": "7811b389-abf8-4950-91a3-ffd969a91d4a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-81a5784ad61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mlogistic_regression_indian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-81a5784ad61a>\u001b[0m in \u001b[0;36mlogistic_regression_indian\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m              \u001b[0;31m# 표준화.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# x = preprocessing.minmax_scale(x)     # 정규화.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'x' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zgLKrxCWxv0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}