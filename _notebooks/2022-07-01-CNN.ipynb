{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-07-01-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# softmax regression 함수 생성\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: \n",
        "\n"
      ],
      "metadata": {
        "id": "CmuoPSXms_Z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10_1_softmax_regression\n",
        "\n"
      ],
      "metadata": {
        "id": "9hbeJ2wZBpxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- softmax 배우기"
      ],
      "metadata": {
        "id": "Z0ehmZA-B7fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "k8skoxrNttBc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "  s = tf.exp(z)\n",
        "  t = tf.reduce_sum(s, axis=1)\n",
        "# print(t.numpy())               [6. 9.]\n",
        "\n",
        "  return s / tf.reshape(t, [-1, 1])\n",
        "\n",
        "# np.sum 은 미분이 잘 안될 가능성이 있음."
      ],
      "metadata": {
        "id": "_CKPKIA3ByHp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- softmax에서 사용하는 cost함수 생성"
      ],
      "metadata": {
        "id": "XsjyeNR1TXlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_entropy(y, p):\n",
        "  loss_i = tf.reduce_sum(y * -tf.math.log(p), axis = 1)\n",
        "  return tf.reduce_mean(loss_i) "
      ],
      "metadata": {
        "id": "nZPW9rxDpKhp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sparse 버전의 cost함수 생성"
      ],
      "metadata": {
        "id": "duL8ob1_MdLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_categorical_cross_entropy(y_sparse, p):\n",
        "  onehot = np.identity(p.shape[-1])\n",
        "  y = onehot[y_sparse]\n",
        "\n",
        "  return categorical_cross_entropy(y, p)"
      ],
      "metadata": {
        "id": "Undz8ih7qPvR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q` 3시간 공부하고 7번 출석한 학생과 5시간 공부하고 2번 출석한 학생의 학점을 구하시오"
      ],
      "metadata": {
        "id": "nr6ZZkrsV1tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_regression():\n",
        "  def Dense(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "  x = [[1, 2],        # C\n",
        "      [2, 1],\n",
        "      [4, 5],         # B\n",
        "      [5, 4],\n",
        "      [8, 9],         # A\n",
        "      [9, 8]]\n",
        "  y = [[0, 0, 1],     # one-hot vector\n",
        "      [0, 0, 1],\n",
        "      [0, 1, 0],\n",
        "      [0, 1, 0],\n",
        "      [1, 0, 0],\n",
        "      [1, 0, 0]]\n",
        "\n",
        "  y = np.int32(y)\n",
        "\n",
        "  w = tf.Variable(tf.random.uniform([2, 3]))   # (6, 3) = (6, 2) * (2, 3)\n",
        "  b = tf.Variable(tf.random.uniform([3]))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "  for i in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "      z = Dense(x, w, b)\n",
        "      hx = keras.activations.softmax(z)\n",
        "      # hx = softmax(z)\n",
        "\n",
        "      loss = categorical_cross_entropy(y, hx)\n",
        "      # cce = keras.losses.CategoricalCrossentropy()\n",
        "      # loss = cce(y, hx)\n",
        "\n",
        "    gradient1 = tape.gradient(loss, [w, b])\n",
        "    optimizer.apply_gradients(zip(gradient1, [w, b]))\n",
        "\n",
        "    print('loss', i,':', loss.numpy())\n",
        "\n",
        "    z = Dense(x, w, b)\n",
        "    p = softmax(z).numpy()\n",
        "\n",
        "\n",
        "    z1 = Dense([[3, 7],\n",
        "                [5, 2]], w, b)\n",
        "    p1 = softmax(z1).numpy()\n",
        "\n",
        "    result = np.argmax(p1, axis=1)\n",
        "    print('3/7, 5/2:', result)\n",
        "\n",
        "    grades = np.array(['A', 'B', 'C'])\n",
        "    print('3/7, 5/2:' , grades[result])\n",
        "\n",
        "softmax_regression()\n",
        "    "
      ],
      "metadata": {
        "id": "Rt0OpOMoMYCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5558cff5-8da0-4bb0-ec0e-e22643849a42"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0 : 1.3614111\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 1 : 1.2378665\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 2 : 1.1659366\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 3 : 1.1236895\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 4 : 1.0977441\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 5 : 1.080907\n",
            "3/7, 5/2: [0 2]\n",
            "3/7, 5/2: ['A' 'C']\n",
            "loss 6 : 1.0693789\n",
            "3/7, 5/2: [0 1]\n",
            "3/7, 5/2: ['A' 'B']\n",
            "loss 7 : 1.0611011\n",
            "3/7, 5/2: [0 1]\n",
            "3/7, 5/2: ['A' 'B']\n",
            "loss 8 : 1.0549096\n",
            "3/7, 5/2: [0 1]\n",
            "3/7, 5/2: ['A' 'B']\n",
            "loss 9 : 1.0501156\n",
            "3/7, 5/2: [0 1]\n",
            "3/7, 5/2: ['A' 'B']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sparse버전 softmax함수를 생성하시오\n"
      ],
      "metadata": {
        "id": "E2I1-AvR92Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_regression_sparse():\n",
        "  def Dense(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "  x = [[1, 2],        # C\n",
        "      [2, 1],\n",
        "      [4, 5],         # B\n",
        "      [5, 4],\n",
        "      [8, 9],         # A\n",
        "      [9, 8]]\n",
        "  y = [2, 2, 1, 1, 0, 0]\n",
        "\n",
        "  y = np.int32(y)\n",
        "\n",
        "  w = tf.Variable(tf.random.uniform([2, 3]))   # (6, 3) = (6, 2) * (2, 3)\n",
        "  b = tf.Variable(tf.random.uniform([3]))\n",
        "# 겉모양만 보기쉽게 숫자로 바뀐 것일뿐, 속 내용은 softmax 버전과 같다. \n",
        "  optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "\n",
        "  for i in range(10):\n",
        "    with tf.GradientTape() as tape:\n",
        "      z = Dense(x, w, b)\n",
        "      hx = softmax(z)\n",
        "\n",
        "      loss = sparse_categorical_cross_entropy(y, hx)\n",
        "      # scce = keras.losses.SparseCategoricalCrossentropy()\n",
        "      # loss = scce(y, hx)\n",
        "\n",
        "    gradient1 = tape.gradient(loss, [w, b])\n",
        "    optimizer.apply_gradients(zip(gradient1, [w, b]))\n",
        "\n",
        "    print('loss', i,':', loss.numpy())\n",
        "\n",
        "    z = Dense(x, w, b)\n",
        "    p = softmax(z).numpy()\n",
        "\n",
        "softmax_regression_sparse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9zwJMrZh1xn",
        "outputId": "7cc43f7a-a171-4dcd-a322-d6a19de7c948"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss 0 : 1.311637\n",
            "loss 1 : 1.2605333\n",
            "loss 2 : 1.2118658\n",
            "loss 3 : 1.1659424\n",
            "loss 4 : 1.1230948\n",
            "loss 5 : 1.0836556\n",
            "loss 6 : 1.0479298\n",
            "loss 7 : 1.0161554\n",
            "loss 8 : 0.9884637\n",
            "loss 9 : 0.9648463\n"
          ]
        }
      ]
    }
  ]
}