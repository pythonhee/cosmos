{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-07-05-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mnist 데이터를 이용한 multi_layers 학습\n",
        "\n",
        "- toc:true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: \n",
        "\n"
      ],
      "metadata": {
        "id": "CmuoPSXms_Z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12_1_single_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "9hbeJ2wZBpxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "k8skoxrNttBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dense 함수와 여러개의 초기화 값 정의하기"
      ],
      "metadata": {
        "id": "J2keY1SRccp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Dense(x, w, b, activations=None):\n",
        "\n",
        "  return activations(x @ w + b) if activations else (x @ w + b)\n",
        "\n",
        "def make_weight_normal(n_input, n_output):\n",
        "  w = tf.Variable(tf.random.uniform([n_input, n_output]))\n",
        "  b = tf.Variable(tf.random.uniform([n_output]))\n",
        "\n",
        "  return w, b\n",
        "\n",
        "def make_weight_glorot(n_input, n_output):\n",
        "  glorot = keras.initializers.GlorotUniform()\n",
        "  w = tf.Variable(glorot([n_input, n_output]))\n",
        "  b = tf.Variable(tf.zeros([n_output]))\n",
        "\n",
        "  return w, b\n",
        "\n",
        "def make_weight_he(n_input, n_output):\n",
        "  he = keras.initializers.HeUniform()\n",
        "  w = tf.Variable(he([n_input, n_output]))\n",
        "  b = tf.Variable(tf.zeros([n_output]))\n",
        "\n",
        "  return w, b\n"
      ],
      "metadata": {
        "id": "N6Kz70tx3xnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mnist_single_layer 함수 생성"
      ],
      "metadata": {
        "id": "E2I1-AvR92Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_single_layer():\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "  print(x_train.shape, x_test.shape)        # (60000, 28, 28) (10000, 28, 28) \n",
        "  print(y_train.shape, y_test.shape)        # (60000,) (10000,) => y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미\n",
        "\n",
        "  x_train = x_train.reshape(-1, 784)\n",
        "  x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "  print(min(x_train[0]), max(x_train[0]))   # 0, 255\n",
        "\n",
        "  x_train = x_train / 255\n",
        "  x_test = x_test / 255\n",
        "\n",
        "# x데이터를 0~1 값으로 정규화시켜주는 작업.\n",
        "\n",
        "  n_classes = 10                              # n_output 값\n",
        "\n",
        "  # w, b = make_weight_normal(x_train.shape[-1], n_classes)\n",
        "  w, b = make_weight_glorot(x_train.shape[-1], n_classes)\n",
        "  # w, b = make_weight_he(x_train.shape[-1], n_classes)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "# SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!\n",
        "\n",
        "  for i in range(100):\n",
        "    with tf.GradientTape() as tape:\n",
        "      hx = Dense(x_train, w, b, activations=keras.activations.softmax)\n",
        "\n",
        "      scce = keras.losses.SparseCategoricalCrossentropy()\n",
        "      loss = scce(y_train, hx)\n",
        "\n",
        "    gradient = tape.gradient(loss, [w, b])\n",
        "    optimizer.apply_gradients(zip(gradient, [w, b]))\n",
        "\n",
        "    print(i, loss.numpy())\n",
        "  print()  \n",
        "\n",
        "  p = Dense(x_test, w, b, activations=keras.activations.softmax)\n",
        "  print(p.numpy().shape)                #(10000, 10)\n",
        "\n",
        "  p_arg = np.argmax(p.numpy(), axis=1)\n",
        "\n",
        "  print(p_arg)\n",
        "\n",
        "  print('acc:', np.mean(p_arg == y_test))"
      ],
      "metadata": {
        "id": "oc4NBqs77Ija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_single_layer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vK5e7tPb7cNs",
        "outputId": "5afc1614-effd-4b3a-b02b-d3e5329497f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000,) (10000,)\n",
            "0 255\n",
            "0 2.4037037\n",
            "1 1.9596609\n",
            "2 1.599312\n",
            "3 1.3130828\n",
            "4 1.1049172\n",
            "5 0.95262057\n",
            "6 0.83855027\n",
            "7 0.7554665\n",
            "8 0.69375753\n",
            "9 0.64428\n",
            "10 0.60332835\n",
            "11 0.5701764\n",
            "12 0.54368937\n",
            "13 0.5217784\n",
            "14 0.502841\n",
            "15 0.48651809\n",
            "16 0.47281685\n",
            "17 0.4612233\n",
            "18 0.45088997\n",
            "19 0.44128928\n",
            "20 0.43242112\n",
            "21 0.42447037\n",
            "22 0.41741672\n",
            "23 0.41099578\n",
            "24 0.4049556\n",
            "25 0.39925858\n",
            "26 0.39401338\n",
            "27 0.3892681\n",
            "28 0.38491824\n",
            "29 0.38080114\n",
            "30 0.376837\n",
            "31 0.37305978\n",
            "32 0.36953098\n",
            "33 0.36624712\n",
            "34 0.36313674\n",
            "35 0.36013457\n",
            "36 0.35723847\n",
            "37 0.35448992\n",
            "38 0.35190997\n",
            "39 0.34946993\n",
            "40 0.34712353\n",
            "41 0.34485543\n",
            "42 0.34268677\n",
            "43 0.34063777\n",
            "44 0.33869678\n",
            "45 0.33683166\n",
            "46 0.33502424\n",
            "47 0.33328217\n",
            "48 0.33161727\n",
            "49 0.33002266\n",
            "50 0.32847792\n",
            "51 0.32697153\n",
            "52 0.32550958\n",
            "53 0.32410192\n",
            "54 0.32274655\n",
            "55 0.32143202\n",
            "56 0.32015175\n",
            "57 0.3189091\n",
            "58 0.317709\n",
            "59 0.31654832\n",
            "60 0.31541887\n",
            "61 0.3143168\n",
            "62 0.3132445\n",
            "63 0.31220457\n",
            "64 0.31119463\n",
            "65 0.31020984\n",
            "66 0.30924878\n",
            "67 0.30831352\n",
            "68 0.30740508\n",
            "69 0.30652076\n",
            "70 0.30565703\n",
            "71 0.3048132\n",
            "72 0.30399042\n",
            "73 0.3031882\n",
            "74 0.30240428\n",
            "75 0.30163667\n",
            "76 0.3008854\n",
            "77 0.3001511\n",
            "78 0.29943293\n",
            "79 0.2987289\n",
            "80 0.2980381\n",
            "81 0.29736075\n",
            "82 0.29669678\n",
            "83 0.29604524\n",
            "84 0.29540515\n",
            "85 0.29477626\n",
            "86 0.2941588\n",
            "87 0.2935524\n",
            "88 0.29295632\n",
            "89 0.29236993\n",
            "90 0.2917933\n",
            "91 0.29122633\n",
            "92 0.2906685\n",
            "93 0.2901193\n",
            "94 0.28957856\n",
            "95 0.28904623\n",
            "96 0.28852212\n",
            "97 0.28800583\n",
            "98 0.2874971\n",
            "99 0.28699583\n",
            "\n",
            "(10000, 10)\n",
            "[7 2 1 ... 4 5 6]\n",
            "acc: 0.9208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- mini_batch 사용해 생성"
      ],
      "metadata": {
        "id": "-cUVVlYVFcSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_single_layer_mini_batch():\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "  print(x_train.shape, x_test.shape)        # (60000, 28, 28) (10000, 28, 28) \n",
        "  print(y_train.shape, y_test.shape)        # (60000,) (10000,) => y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미\n",
        "\n",
        "  x_train = x_train.reshape(-1, 784)\n",
        "  x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "  print(min(x_train[0]), max(x_train[0]))   # 0, 255\n",
        "\n",
        "  x_train = x_train / 255\n",
        "  x_test = x_test / 255\n",
        "\n",
        "# x데이터를 0~1 값으로 정규화시켜주는 작업.\n",
        "\n",
        "  n_classes = 10                              # n_output 값\n",
        "\n",
        "  w, b = make_weight_normal(x_train.shape[-1], n_classes)\n",
        "\n",
        "  # w, b = make_weight_glorot(x_train.shape[-1], n_classes)\n",
        "  # acc: 0.4583\n",
        "  \n",
        "  # w, b = make_weight_he(x_train.shape[-1], n_classes)\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "# SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!\n",
        "\n",
        "  epoch = 10                               # 60000데이터를 10회 반복한다는 의미 \n",
        "  batch_size = 100\n",
        "  n_literation = len(x_train) // batch_size   # 600\n",
        "\n",
        "  for i in range(epoch):\n",
        "    total = 0\n",
        "    for j in range(n_literation):              # 6000번이나 반복 확률이 up\n",
        "      n1 = j * batch_size\n",
        "      n2 = n1 * batch_size\n",
        "\n",
        "      xx = x_train[n1:n2]\n",
        "      yy = y_train[n1:n2]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        hx = Dense(xx, w, b, activations=keras.activations.softmax)\n",
        "\n",
        "        scce = keras.losses.SparseCategoricalCrossentropy()\n",
        "        loss = scce(yy, hx)\n",
        "        total += loss.numpy()\n",
        "\n",
        "      gradient = tape.gradient(loss, [w, b])\n",
        "      optimizer.apply_gradients(zip(gradient, [w, b]))\n",
        "\n",
        "    print(i, total / n_literation)\n",
        "  print()  \n",
        "\n",
        "  p = Dense(x_test, w, b, activations=keras.activations.softmax)\n",
        "  print(p.numpy().shape)                #(10000, 10)\n",
        "\n",
        "  p_arg = np.argmax(p.numpy(), axis=1)\n",
        "\n",
        "  print(p_arg)\n",
        "\n",
        "  print('acc:', np.mean(p_arg == y_test))\n"
      ],
      "metadata": {
        "id": "56c8is4VFpC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_single_layer_mini_batch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEKdfxgXHcjy",
        "outputId": "50d0e918-0aab-4b39-90b1-0894b807bfdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000,) (10000,)\n",
            "0 255\n",
            "0 0.31139886550294854\n",
            "1 0.1945932127473255\n",
            "2 0.17942959094420075\n",
            "3 0.17137116949539632\n",
            "4 0.1662351022940129\n",
            "5 0.16266893739035973\n",
            "6 0.16009065205580555\n",
            "7 0.15819382834791515\n",
            "8 0.15678756201523356\n",
            "9 0.1557471169391647\n",
            "\n",
            "(10000, 10)\n",
            "[7 2 1 ... 4 5 6]\n",
            "acc: 0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12_2_multi_layers"
      ],
      "metadata": {
        "id": "fQtqibATbGde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "oMst2hS5hpYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- layer 3개"
      ],
      "metadata": {
        "id": "NPd1MNjShzdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_multiple_layer_mini_batch_3():\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "  print(x_train.shape, x_test.shape)        # (60000, 28, 28) (10000, 28, 28) \n",
        "  print(y_train.shape, y_test.shape)        # (60000,) (10000,) => y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미\n",
        "\n",
        "  x_train = x_train.reshape(-1, 784)\n",
        "  x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "  print(min(x_train[0]), max(x_train[0]))   # 0, 255\n",
        "\n",
        "  x_train = x_train / 255\n",
        "  x_test = x_test / 255\n",
        "\n",
        "# x데이터를 0~1 값으로 정규화시켜주는 작업.\n",
        "\n",
        "  n_classes = 10                              # n_output 값\n",
        "\n",
        "  w1, b1 = make_weight_normal(x_train.shape[-1], 256)\n",
        "  w2, b2 = make_weight_normal(256, 256)\n",
        "  w3, b3 = make_weight_normal(256, n_classes)\n",
        "  # acc: 0.9423\n",
        "\n",
        "  # w1, b1 = make_weight_glorot(x_train.shape[-1], 256)\n",
        "  # w2, b2 = make_weight_glorot(256, 256)\n",
        "  # w3, b3 = make_weight_glorot(256, n_classes)\n",
        "  # 결과 :\n",
        "\n",
        "  # w1, b1 = make_weight_he(x_train.shape[-1], 256)\n",
        "  # w2, b2 = make_weight_he(256, 256)\n",
        "  # w3, b3 = make_weight_he(256, n_classes)\n",
        "  # 결과 :\n",
        "\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "# SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!\n",
        "\n",
        "  epoch = 10                               # 60000데이터를 10회 반복한다는 의미 \n",
        "  batch_size = 100\n",
        "  n_literation = len(x_train) // batch_size   # 600\n",
        "\n",
        "  for i in range(epoch):\n",
        "    total = 0\n",
        "    for j in range(n_literation):              # 6000번이나 반복 확률이 up\n",
        "      n1 = j * batch_size\n",
        "      n2 = n1 * batch_size\n",
        "\n",
        "      xx = x_train[n1:n2]\n",
        "      yy = y_train[n1:n2]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        d1 = Dense(xx, w1, b1, activations=keras.activations.relu)\n",
        "        d2 = Dense(d1, w2, b2, activations=keras.activations.relu)\n",
        "        hx = Dense(d2, w3, b3, activations=keras.activations.softmax)\n",
        "\n",
        "  # layer끼리 연결할 때 activation은 non-linear해야한다. \n",
        "  # sigmoid ,  softmax 는 예측 x = > 마지막에만 확률로 변경하는 sigmoid, softmax 사용\n",
        "  # relu 는 0보다 작은 값은 0으로, 0보다 큰 값은 그대로 나타냄 0보다 작은 값이 많은 데이터는 사용하기 어려움.\n",
        "\n",
        "        scce = keras.losses.SparseCategoricalCrossentropy()\n",
        "        loss = scce(yy, hx)\n",
        "        total += loss.numpy()\n",
        "\n",
        "      gradient = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n",
        "      optimizer.apply_gradients(zip(gradient, [w1, b1, w2, b2, w3, b3]))\n",
        "\n",
        "    print(i, total / n_literation)\n",
        "  print()  \n",
        "\n",
        "  d1 = Dense(x_test, w1, b1, activations=keras.activations.relu)\n",
        "  d2 = Dense(d1, w2, b2, activations=keras.activations.relu)\n",
        "  p = Dense(d2, w3, b3, activations=keras.activations.softmax)\n",
        "  print(p.numpy().shape)                #(10000, 10)\n",
        "\n",
        "  p_arg = np.argmax(p.numpy(), axis=1)\n",
        "\n",
        "  print('acc:', np.mean(p_arg == y_test))"
      ],
      "metadata": {
        "id": "WVQCFK1khzJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_multiple_layer_mini_batch_3()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCNqnuWfKDUF",
        "outputId": "72c9bd26-f70e-4747-94ba-5990acf619df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000,) (10000,)\n",
            "0 255\n",
            "0 1461.6672620370766\n",
            "1 1.1854659057674386\n",
            "2 0.3881142702441624\n",
            "3 0.15620899812240774\n",
            "4 0.06981539531475088\n",
            "5 0.03489897323093222\n",
            "6 3.321667264445374\n",
            "7 0.14723950815076628\n",
            "8 0.09502428941739102\n",
            "9 0.06494564253681649\n",
            "\n",
            "(10000, 10)\n",
            "acc: 0.9423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q` layer 5개로 늘려 정확도를 구하고 layer 3개를 이용했을 때와 비교하시오"
      ],
      "metadata": {
        "id": "B2cUB0Bch_vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mnist_multiple_layer_mini_batch_5():\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "  print(x_train.shape, x_test.shape)        # (60000, 28, 28) (10000, 28, 28) \n",
        "  print(y_train.shape, y_test.shape)        # (60000,) (10000,) => y값이 1차원이라는 뜻은 sparse 버전으로 되어있다는 의미\n",
        "\n",
        "  x_train = x_train.reshape(-1, 784)\n",
        "  x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "  print(min(x_train[0]), max(x_train[0]))   # 0, 255\n",
        "\n",
        "  x_train = x_train / 255\n",
        "  x_test = x_test / 255\n",
        "\n",
        "# x데이터를 0~1 값으로 정규화시켜주는 작업.\n",
        "\n",
        "  n_classes = 10                              # n_output 값\n",
        "\n",
        "  w1, b1 = make_weight_normal(x_train.shape[-1], 256)\n",
        "  w2, b2 = make_weight_normal(256, 64)\n",
        "  w3, b3 = make_weight_normal(64, 32)\n",
        "  w4, b4 = make_weight_normal(32, 32)\n",
        "  w5, b5 = make_weight_normal(32, n_classes)\n",
        "  # acc: 0.8988 \n",
        "\n",
        "  # w1, b1 = make_weight_glorot(x_train.shape[-1], 256)\n",
        "  # w2, b2 = make_weight_glorot(256, 64)\n",
        "  # w3, b3 = make_weight_glorot(64, 32)\n",
        "  # w4, b4 = make_weight_glorot(32, 32)\n",
        "  # w5, b5 = make_weight_glorot(32, n_classes)\n",
        "  # 결과 :\n",
        "\n",
        "  # w1, b1 = make_weight_he(x_train.shape[-1], 256)\n",
        "  # w2, b2 = make_weight_he(256, 64)\n",
        "  # w3, b3 = make_weight_he(64, 32)\n",
        "  # w4, b4 = make_weight_he(32, 32)\n",
        "  # w5, b5 = make_weight_he(32, n_classes)\n",
        "  # 결과 :\n",
        "\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "# SGD, RMSprop 써도 괜찮음 여러 개 써가면서 성능 비교!\n",
        "\n",
        "  epoch = 10                               # 60000데이터를 10회 반복한다는 의미 \n",
        "  batch_size = 100\n",
        "  n_literation = len(x_train) // batch_size   # 600\n",
        "\n",
        "  for i in range(epoch):\n",
        "    total = 0\n",
        "    for j in range(n_literation):              # 6000번이나 반복 확률이 up\n",
        "      n1 = j * batch_size\n",
        "      n2 = n1 * batch_size\n",
        "\n",
        "      xx = x_train[n1:n2]\n",
        "      yy = y_train[n1:n2]\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        d1 = Dense(xx, w1, b1, activations=keras.activations.relu)\n",
        "        d2 = Dense(d1, w2, b2, activations=keras.activations.relu)\n",
        "        d3 = Dense(d2, w3, b3, activations=keras.activations.relu)\n",
        "        d4 = Dense(d3, w4, b4, activations=keras.activations.relu)\n",
        "        hx = Dense(d4, w5, b5, activations=keras.activations.softmax)\n",
        "\n",
        "        scce = keras.losses.SparseCategoricalCrossentropy()\n",
        "        loss = scce(yy, hx)\n",
        "        total += loss.numpy()\n",
        "\n",
        "      gradient = tape.gradient(loss, [w1, b1, w2, b2, w3, b3, w4, b4, w5, b5])\n",
        "      optimizer.apply_gradients(zip(gradient, [w1, b1, w2, b2, w3, b3, w4, b4, w5, b5]))\n",
        "\n",
        "    print(i, total / n_literation)\n",
        "  print()  \n",
        "\n",
        "  d1 = Dense(x_test, w1, b1, activations=keras.activations.relu)\n",
        "  d2 = Dense(d1, w2, b2, activations=keras.activations.relu)\n",
        "  d3 = Dense(d2, w3, b3, activations=keras.activations.relu)\n",
        "  d4 = Dense(d3, w4, b4, activations=keras.activations.relu)\n",
        "  p = Dense(d4, w5, b5, activations=keras.activations.softmax)\n",
        "  print(p.numpy().shape)                #(10000, 10)\n",
        "\n",
        "  p_arg = np.argmax(p.numpy(), axis=1)\n",
        "\n",
        "  print('acc:', np.mean(p_arg == y_test))"
      ],
      "metadata": {
        "id": "kZ5eZ2RKhrwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_multiple_layer_mini_batch_5()"
      ],
      "metadata": {
        "id": "KBE3HQg8imhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d2f71d-f7b8-4e1b-b6b8-ffdb616c0a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (10000, 28, 28)\n",
            "(60000,) (10000,)\n",
            "0 255\n",
            "0 115397.61487060547\n",
            "1 88.93705628871918\n",
            "2 124.64481646577518\n",
            "3 8.39682581782341\n",
            "4 95.10231856604418\n",
            "5 5.290038377592961\n",
            "6 3.5611470832582564\n",
            "7 8.595668331881365\n",
            "8 25.354680725370223\n",
            "9 1.1293716185198477\n",
            "\n",
            "(10000, 10)\n",
            "acc: 0.8988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Q` normal 버전 말고 glorot, he 버전도 돌려보면서 정확도를 측정해 보시오 "
      ],
      "metadata": {
        "id": "oCHwUadqVidu"
      }
    }
  ]
}